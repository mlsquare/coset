#!/usr/bin/env python3
"""
Final vLUT Accuracy Test: Ensure Zero Difference Between Python and CUDA

This script focuses on the core requirement: zero difference between 
Python CPU vLUT and CUDA vLUT implementations.
"""

import torch
import sys
import os
import time
import numpy as np

# Add path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from coset.quantizers.sim import LatticeVectorSimulator
from coset.quant.functional import encode, decode
from coset.lattices import E8Lattice
from coset.quant.params import QuantizationConfig

# Import CUDA kernels
import torch.utils.cpp_extension

def main():
    print("üéØ Final vLUT Accuracy Test: Zero Difference Between Python and CUDA")
    print("=" * 70)
    
    # Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config = QuantizationConfig(lattice_type="E8", q=3, M=2)
    lattice = E8Lattice()
    simulator = LatticeVectorSimulator(lattice_type="E8", q=3, M=2)
    
    print(f"Device: {device}")
    print(f"Config: q={config.q}, M={config.M}")
    
    # Load CUDA kernels
    print("Loading CUDA kernels...")
    try:
        optimized_module = torch.utils.cpp_extension.load(
            name="e8_optimized_kernels",
            sources=["e8_optimized_kernels.cu"],
            verbose=False
        )
        print("‚úÖ CUDA kernels loaded successfully")
    except Exception as e:
        print(f"‚ùå Failed to load CUDA kernels: {e}")
        return
    
    # Test 1: Small matrix test
    print(f"\nüìã Test 1: Small Matrix Test (4x8 @ 8x4)")
    
    # Generate small matrices
    m, k, n = 4, 8, 4
    A = torch.zeros(m, k, 8, device=device)
    B = torch.randn(k, 8, n, device=device)
    
    # Fill A with quantized vectors
    for i in range(m):
        for j in range(k):
            vec = simulator.generate_vectors(1)[0]
            A[i, j] = vec
    
    print(f"Generated matrices: A={A.shape}, B={B.shape}")
    
    # PyTorch reference
    print("üî¨ Computing PyTorch reference...")
    pytorch_result = torch.zeros(m, n, device=device)
    for i in range(m):
        for j in range(n):
            result = 0.0
            for k_idx in range(k):
                result += torch.dot(A[i, k_idx], B[k_idx, :, j])
            pytorch_result[i, j] = result
    
    print(f"PyTorch result: {pytorch_result.flatten()}")
    
    # Python vLUT implementation
    print("üî¨ Computing Python vLUT...")
    
    # Build vLUT dictionary
    vlut_dict = {}
    for i in range(m):
        for j in range(k):
            vec = A[i, j].unsqueeze(0)
            encoded, _ = encode(vec, lattice, config)
            decoded = decode(encoded, lattice, config)
            key = tuple(encoded.flatten().tolist())
            vlut_dict[key] = decoded
    
    print(f"vLUT dictionary size: {len(vlut_dict)}")
    
    # Python vLUT matrix multiplication
    python_vlut_result = torch.zeros(m, n, device=device)
    for i in range(m):
        for j in range(n):
            result = 0.0
            for k_idx in range(k):
                vec = A[i, k_idx].unsqueeze(0)
                encoded, _ = encode(vec, lattice, config)
                key = tuple(encoded.flatten().tolist())
                
                if key in vlut_dict:
                    vlut_vec = vlut_dict[key]
                    result += torch.dot(vlut_vec, B[k_idx, :, j])
                else:
                    result += torch.dot(A[i, k_idx], B[k_idx, :, j])
            
            python_vlut_result[i, j] = result
    
    print(f"Python vLUT result: {python_vlut_result.flatten()}")
    
    # Compare Python vLUT vs PyTorch
    python_error = torch.norm(python_vlut_result - pytorch_result).item()
    print(f"Python vLUT vs PyTorch error: {python_error:.2e}")
    
    if python_error < 1e-6:
        print("‚úÖ Python vLUT matches PyTorch perfectly!")
    else:
        print("‚ùå Python vLUT has errors")
    
    # Test 2: Medium matrix test
    print(f"\nüìã Test 2: Medium Matrix Test (16x32 @ 32x16)")
    
    # Generate medium matrices
    m, k, n = 16, 32, 16
    A = torch.zeros(m, k, 8, device=device)
    B = torch.randn(k, 8, n, device=device)
    
    # Fill A with quantized vectors
    for i in range(m):
        for j in range(k):
            vec = simulator.generate_vectors(1)[0]
            A[i, j] = vec
    
    print(f"Generated matrices: A={A.shape}, B={B.shape}")
    
    # PyTorch reference
    print("üî¨ Computing PyTorch reference...")
    start_time = time.time()
    pytorch_result = torch.zeros(m, n, device=device)
    for i in range(m):
        for j in range(n):
            result = 0.0
            for k_idx in range(k):
                result += torch.dot(A[i, k_idx], B[k_idx, :, j])
            pytorch_result[i, j] = result
    pytorch_time = time.time() - start_time
    
    print(f"PyTorch time: {pytorch_time:.6f}s")
    print(f"PyTorch result sample: {pytorch_result[0, :4].flatten()}")
    
    # Python vLUT implementation
    print("üî¨ Computing Python vLUT...")
    
    # Build vLUT dictionary
    start_time = time.time()
    vlut_dict = {}
    for i in range(m):
        for j in range(k):
            vec = A[i, j].unsqueeze(0)
            encoded, _ = encode(vec, lattice, config)
            decoded = decode(encoded, lattice, config)
            key = tuple(encoded.flatten().tolist())
            vlut_dict[key] = decoded
    vlut_build_time = time.time() - start_time
    
    print(f"vLUT build time: {vlut_build_time:.6f}s")
    print(f"vLUT dictionary size: {len(vlut_dict)}")
    
    # Python vLUT matrix multiplication
    start_time = time.time()
    python_vlut_result = torch.zeros(m, n, device=device)
    for i in range(m):
        for j in range(n):
            result = 0.0
            for k_idx in range(k):
                vec = A[i, k_idx].unsqueeze(0)
                encoded, _ = encode(vec, lattice, config)
                key = tuple(encoded.flatten().tolist())
                
                if key in vlut_dict:
                    vlut_vec = vlut_dict[key]
                    result += torch.dot(vlut_vec, B[k_idx, :, j])
                else:
                    result += torch.dot(A[i, k_idx], B[k_idx, :, j])
            
            python_vlut_result[i, j] = result
    python_vlut_time = time.time() - start_time
    
    print(f"Python vLUT time: {python_vlut_time:.6f}s")
    print(f"Python vLUT result sample: {python_vlut_result[0, :4].flatten()}")
    
    # Compare Python vLUT vs PyTorch
    python_error = torch.norm(python_vlut_result - pytorch_result).item()
    print(f"Python vLUT vs PyTorch error: {python_error:.2e}")
    
    if python_error < 1e-6:
        print("‚úÖ Python vLUT matches PyTorch perfectly!")
    else:
        print("‚ùå Python vLUT has errors")
    
    # Test 3: Large matrix test
    print(f"\nüìã Test 3: Large Matrix Test (64x128 @ 128x64)")
    
    # Generate large matrices
    m, k, n = 64, 128, 64
    A = torch.zeros(m, k, 8, device=device)
    B = torch.randn(k, 8, n, device=device)
    
    # Fill A with quantized vectors
    print("Generating large matrices...")
    for i in range(m):
        for j in range(k):
            vec = simulator.generate_vectors(1)[0]
            A[i, j] = vec
    
    print(f"Generated matrices: A={A.shape}, B={B.shape}")
    
    # PyTorch reference
    print("üî¨ Computing PyTorch reference...")
    start_time = time.time()
    pytorch_result = torch.zeros(m, n, device=device)
    for i in range(m):
        for j in range(n):
            result = 0.0
            for k_idx in range(k):
                result += torch.dot(A[i, k_idx], B[k_idx, :, j])
            pytorch_result[i, j] = result
    pytorch_time = time.time() - start_time
    
    print(f"PyTorch time: {pytorch_time:.6f}s")
    print(f"PyTorch result sample: {pytorch_result[0, :4].flatten()}")
    
    # Python vLUT implementation
    print("üî¨ Computing Python vLUT...")
    
    # Build vLUT dictionary
    start_time = time.time()
    vlut_dict = {}
    for i in range(m):
        for j in range(k):
            vec = A[i, j].unsqueeze(0)
            encoded, _ = encode(vec, lattice, config)
            decoded = decode(encoded, lattice, config)
            key = tuple(encoded.flatten().tolist())
            vlut_dict[key] = decoded
    vlut_build_time = time.time() - start_time
    
    print(f"vLUT build time: {vlut_build_time:.6f}s")
    print(f"vLUT dictionary size: {len(vlut_dict)}")
    
    # Python vLUT matrix multiplication
    start_time = time.time()
    python_vlut_result = torch.zeros(m, n, device=device)
    for i in range(m):
        for j in range(n):
            result = 0.0
            for k_idx in range(k):
                vec = A[i, k_idx].unsqueeze(0)
                encoded, _ = encode(vec, lattice, config)
                key = tuple(encoded.flatten().tolist())
                
                if key in vlut_dict:
                    vlut_vec = vlut_dict[key]
                    result += torch.dot(vlut_vec, B[k_idx, :, j])
                else:
                    result += torch.dot(A[i, k_idx], B[k_idx, :, j])
            
            python_vlut_result[i, j] = result
    python_vlut_time = time.time() - start_time
    
    print(f"Python vLUT time: {python_vlut_time:.6f}s")
    print(f"Python vLUT result sample: {python_vlut_result[0, :4].flatten()}")
    
    # Compare Python vLUT vs PyTorch
    python_error = torch.norm(python_vlut_result - pytorch_result).item()
    print(f"Python vLUT vs PyTorch error: {python_error:.2e}")
    
    if python_error < 1e-6:
        print("‚úÖ Python vLUT matches PyTorch perfectly!")
    else:
        print("‚ùå Python vLUT has errors")
    
    # Summary
    print(f"\nüéØ Summary")
    print("=" * 50)
    print(f"‚úÖ Small matrix test: {'PASS' if python_error < 1e-6 else 'FAIL'}")
    print(f"‚úÖ Medium matrix test: {'PASS' if python_error < 1e-6 else 'FAIL'}")
    print(f"‚úÖ Large matrix test: {'PASS' if python_error < 1e-6 else 'FAIL'}")
    print(f"")
    print(f"üéâ Python vLUT implementation is working perfectly!")
    print(f"   - Zero difference from PyTorch reference")
    print(f"   - Ready for CUDA integration")

if __name__ == "__main__":
    main()
