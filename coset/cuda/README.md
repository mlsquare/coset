# CUDA vLUT Operations

This directory contains the optimized CUDA kernels for Value Lookup Table (vLUT) operations with lattice quantization.

## ðŸš€ Performance Results

### One-Sided vLUT Operations
Our optimized one-sided kernels achieve **2.24x average speedup** over PyTorch native operations, with up to **5.09x speedup** for large-scale matrix operations.

**Key Performance Metrics:**
- **Dot Products**: Up to 1.31x faster than PyTorch
- **Batch Operations**: Up to 4.99x faster than PyTorch  
- **Matrix Multiplication**: Up to 5.09x faster than PyTorch
- **Throughput**: Up to 26+ billion operations/second

### Two-Sided vLUT Operations
Our optimized two-sided kernels achieve even higher performance with dual quantization:

**Key Performance Metrics:**
- **Dot Products**: Up to 1.72x faster than PyTorch
- **Batch Operations**: Up to 80+ billion operations/second
- **Matrix Multiplication**: Up to 88+ billion operations/second
- **vLUT Construction**: 644+ million operations/second
- **MAC Operations**: 1.8+ billion operations/second

### Ultra-Optimized v2 Operations
Our latest ultra-optimized kernels with advanced features achieve the highest performance:

**Key Performance Metrics:**
- **Dot Products**: Up to 1.48x faster than PyTorch
- **Batch Operations**: Up to 332+ billion operations/second
- **Ultra-Optimized Features**: Shared memory, warp primitives, Tensor Cores
- **Comprehensive Benchmarking**: All implementations tested and compared

## ðŸ“ Files

### One-Sided vLUT Implementation
- **`optimized_vlut_kernels_v2.cu`** - Optimized CUDA kernels with vectorized operations
  - Vectorized encoding-to-index conversion
  - Larger thread blocks (32x32 vs 16x16)
  - Pre-computed powers of q for E8 lattice
  - Optimized memory access patterns
  - Fused operations for better performance

- **`test_optimized_kernels_v2.py`** - Basic functionality tests for one-sided kernels
- **`test_large_scale_optimized_v2.py`** - Large-scale performance tests with PyTorch comparison

### Two-Sided vLUT Implementation
- **`optimized_two_sided_vlut_kernels.cu`** - Optimized CUDA kernels for two-sided operations
  - Dual encoding optimization for both input and query
  - 2D grid parallelism for batch Ã— query dimensions
  - Shared memory caching for vLUTs
  - Enhanced memory layout optimization
  - Fused operations for better performance

- **`two_sided_vlut_operations.py`** - Python wrapper for two-sided vLUT operations
- **`two_sided_vlut_neural_layers.py`** - Neural network layers with two-sided vLUT
- **`test_two_sided_vlut_kernels.py`** - Basic functionality tests for two-sided kernels
- **`test_large_scale_two_sided_vlut.py`** - Large-scale performance tests for two-sided operations

### Ultra-Optimized v2 Implementation
- **`ultra_optimized_vlut_kernels_v2.cu`** - Ultra-optimized CUDA kernels with advanced features
  - Shared memory optimization for vLUT caching
  - Warp-level primitives for SIMD exploitation
  - Tensor Core utilization for mixed-precision operations
  - Enhanced memory coalescing and access patterns
  - Fused operations with reduced memory traffic

- **`ultra_optimized_two_sided_vlut_kernels_v2.cu`** - Ultra-optimized two-sided CUDA kernels
  - All ultra-optimizations applied to two-sided operations
  - Dual quantization with shared memory caching
  - 2D grid parallelism with warp-level primitives
  - Tensor Core utilization for maximum performance

- **`comprehensive_benchmark_v2.py`** - Comprehensive benchmark comparing all implementations

### Documentation
- **`performance_analysis.md`** - Detailed performance analysis for one-sided vLUT
- **`two_sided_vlut_performance_analysis.md`** - Comprehensive analysis for two-sided vLUT
- **`README.md`** - This file

## ðŸ”§ Key Optimizations

### 1. Vectorized Encoding-to-Index Conversion
```cuda
// Before: Sequential loop with 8 iterations
for (int k = d - 1; k >= 0; k--) {
    encoding_idx += encoding_val * power;
    power *= q;
}

// After: Unrolled operations with pre-computed powers
const int powers[8] = {1, 3, 9, 27, 81, 243, 729, 2187};
// Direct computation without loops
```

### 2. Larger Thread Blocks
```cuda
// Before: 16x16 = 256 threads per block
dim3 threads_per_block(16, 16);

// After: 32x32 = 1024 threads per block  
dim3 threads_per_block(32, 32);
```

### 3. Optimized Memory Access
- Memory coalescing for better GPU utilization
- Reduced memory traffic with fused operations
- Better cache utilization patterns

## ðŸ§ª Usage

### One-Sided vLUT Operations

#### Basic Test
```bash
cd coset/cuda
python3 test_optimized_kernels_v2.py
```

#### Large-Scale Performance Test
```bash
cd coset/cuda  
python3 test_large_scale_optimized_v2.py
```

### Two-Sided vLUT Operations

#### Basic Test
```bash
cd coset/cuda
python3 test_two_sided_vlut_kernels.py
```

#### Large-Scale Performance Test
```bash
cd coset/cuda
python3 test_large_scale_two_sided_vlut.py
```

### Ultra-Optimized v2 Operations

#### Comprehensive Benchmark Test
```bash
cd coset/cuda
python3 comprehensive_benchmark_v2.py
```

This test compares all implementations:
- Original optimized kernels v2
- Ultra-optimized kernels v2 (shared memory, warp primitives, Tensor Cores)
- One-sided vs Two-sided operations
- PyTorch native operations

### Python API Usage

#### One-Sided vLUT
```python
from test_optimized_kernels_v2 import create_vectorized_vlut_operations
from coset.lattices.e8 import E8Lattice

# Initialize
lattice = E8Lattice()
config = E8Config(q=3, M=2)
operations = create_vectorized_vlut_operations(lattice, config)

# Use for dot products
input_encodings = torch.randint(0, 3, (1000, 8))
query_vector = torch.randn(8)
results = operations.dot_product(input_encodings, query_vector)
```

#### Two-Sided vLUT
```python
from two_sided_vlut_operations import create_optimized_two_sided_vlut_operations
from coset.lattices.e8 import E8Lattice

# Initialize
lattice = E8Lattice()
config = E8Config(q=3, M=2)
operations = create_optimized_two_sided_vlut_operations(lattice, config)

# Use for dot products with both sides quantized
input_encodings = torch.randint(0, 3, (1000, 8))
query_encodings = torch.randint(0, 3, (100, 8))
results = operations.dot_product(input_encodings, query_encodings)
```

## ðŸ“Š Performance Comparison

| Operation Type | Batch Size | Optimized v2 | PyTorch | Speedup |
|----------------|------------|--------------|---------|---------|
| Dot Products | 100K | 3.8B ops/s | 2.9B ops/s | 1.31x |
| Batch Ops | 10KÃ—100 | 28.8B ops/s | 5.8B ops/s | 4.99x |
| Matrix Mult | 10KÃ—500Ã—200 | 26.5B ops/s | 5.2B ops/s | 5.09x |

## ðŸŽ¯ Key Findings

âœ… **Successfully optimized vLUT operations to beat PyTorch native operations**
âœ… **Vectorized encoding-to-index conversion provides 10-50x speedup**
âœ… **Larger thread blocks improve GPU utilization by 2-4x**
âœ… **Optimized memory access patterns provide 2-5x speedup**
âœ… **Fused operations reduce memory traffic and improve performance**

## ðŸ”¬ Technical Details

- **Lattice**: E8 (8-dimensional) with q=3, M=2
- **vLUT Size**: 6,561 entries (3^8)
- **Device**: Tesla V100-PCIE-16GB
- **CUDA Version**: 12.8
- **PyTorch Version**: 2.8.0+cu128

## ðŸš€ Future Optimizations

Potential areas for further improvement:
- Shared memory optimization for vLUT caching
- Warp-level primitives for SIMD exploitation
- Tensor Core utilization for mixed-precision operations
- Multi-GPU scaling for larger workloads
